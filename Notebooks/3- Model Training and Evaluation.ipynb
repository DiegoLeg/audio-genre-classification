{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed data from the previous notebook is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r x_train \n",
    "%store -r x_test \n",
    "%store -r y_train \n",
    "%store -r y_test \n",
    "%store -r yy \n",
    "%store -r label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first place, a K-Nearest Neighbor algorithm is tested. In order to find k, GridSearch is used with a series of parameters to evaluate. Using GridSearch we can evaluate all the possible combinations of the hyperparameters values using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining grid parameters for the algorithm to test\n",
    "\n",
    "grid_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': [3, 5, 7, 9, 11, 15], 'metric': ['euclidean', 'manhattan'], 'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = GridSearchCV(KNeighborsClassifier(), grid_params, scoring='accuracy',cv=5)\n",
    "model_knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the accuracy of our model and also the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Model Score', 0.535)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[15,  0,  2,  1,  0,  1,  1,  0,  0,  0],\n",
       "       [ 1, 12,  0,  0,  0,  2,  0,  0,  0,  1],\n",
       "       [ 0,  0, 14,  2,  2,  1,  1,  0,  2,  3],\n",
       "       [ 0,  0,  2,  9,  2,  1,  0,  2,  3,  2],\n",
       "       [ 1,  0,  1,  3,  3,  0,  2,  1,  4,  1],\n",
       "       [ 1,  1,  3,  0,  0, 13,  0,  1,  2,  0],\n",
       "       [ 1,  0,  1,  1,  2,  1, 20,  0,  0,  1],\n",
       "       [ 0,  0,  1,  1,  0,  0,  0,  6,  1,  1],\n",
       "       [ 0,  0,  2,  2,  6,  1,  0,  2,  8,  5],\n",
       "       [ 1,  0,  1,  2,  0,  2,  1,  1,  3,  7]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model score: Accuracy\n",
    "knn_score = model_knn.score(x_test, y_test)\n",
    "print (\"Model Score\",  knn_score)\n",
    "\n",
    "y_predict_knn = model_knn.predict(x_test)\n",
    "\n",
    "#Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_predict_knn, y_test)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameters found by GridSearch\n",
    "model_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the first model does not perform as we expected, with an accuracy of 0.53. Let's try with another models before changing some parameters in the preprocessing notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple Decision Tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=10,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a decision tree model\n",
    "\n",
    "model_tree = DecisionTreeClassifier(random_state=10)\n",
    "model_tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to predict the labels for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  1,  6,  1,  0,  1,  1,  0,  0,  4],\n",
       "       [ 0, 11,  3,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  8,  0,  1,  4,  0,  0,  4,  0],\n",
       "       [ 1,  0,  2,  5,  2,  1,  1,  2,  2,  3],\n",
       "       [ 0,  0,  1,  3,  2,  1,  2,  0,  6,  3],\n",
       "       [ 3,  1,  2,  0,  0, 10,  0,  0,  2,  3],\n",
       "       [ 2,  0,  1,  2,  2,  0, 18,  0,  2,  1],\n",
       "       [ 0,  0,  0,  1,  2,  0,  0,  8,  1,  0],\n",
       "       [ 0,  0,  0,  3,  5,  2,  1,  1,  5,  1],\n",
       "       [ 3,  0,  4,  6,  1,  3,  2,  2,  1,  6]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_tree = model_tree.predict(x_test)\n",
    "\n",
    "#class_rep_tree = classification_report(y_test, predict_labels_tree)\n",
    "conf_matrix_tree = confusion_matrix(y_predict, y_test)\n",
    "conf_matrix_tree\n",
    "\n",
    "\n",
    "#print(\"Decision Tree:\", class_rep_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predict_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that Accuracy has not improved in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what can we do with a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forest = RandomForestClassifier(n_estimators=100, max_depth= 5)\n",
    "model_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forest.score (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  1  0  0  0  1  6  0  0  4]\n",
      " [ 0 13  0  0  0  0  0  0  0  0]\n",
      " [ 7  1  4  2  1  5  1  2  0  4]\n",
      " [ 0  0  0  9  3  0  1  5  1  2]\n",
      " [ 0  0  0  0  3  0  2  3  5  2]\n",
      " [ 5  1  3  2  0  5  1  1  4  0]\n",
      " [ 1  0  0  0  1  0 22  0  0  1]\n",
      " [ 0  0  0  2  0  0  0  8  3  0]\n",
      " [ 1  1  1  2  3  0  2  2  9  2]\n",
      " [ 4  0  1  4  0  0  1  2  4  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.40      0.35        20\n",
      "           1       0.76      1.00      0.87        13\n",
      "           2       0.44      0.15      0.22        27\n",
      "           3       0.43      0.43      0.43        21\n",
      "           4       0.27      0.20      0.23        15\n",
      "           5       0.45      0.23      0.30        22\n",
      "           6       0.61      0.88      0.72        25\n",
      "           7       0.35      0.62      0.44        13\n",
      "           8       0.35      0.39      0.37        23\n",
      "           9       0.25      0.24      0.24        21\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       200\n",
      "   macro avg       0.42      0.45      0.42       200\n",
      "weighted avg       0.42      0.43      0.40       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_forest = model_forest.predict(x_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_predict_forest)\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_test, y_predict_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved a similar accuracy to the obtained in the Decision Tree model. As a Random Forest is a more robust algorithm, we can start thinking we can improve our preprocessing stage to improve this metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will compare the performance of our models to a Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/diego/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=10, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logreg = LogisticRegression(random_state=10)\n",
    "model_logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.50      0.35        20\n",
      "           1       0.81      1.00      0.90        13\n",
      "           2       0.53      0.33      0.41        27\n",
      "           3       0.42      0.38      0.40        21\n",
      "           4       0.31      0.27      0.29        15\n",
      "           5       0.58      0.32      0.41        22\n",
      "           6       0.71      0.88      0.79        25\n",
      "           7       0.47      0.69      0.56        13\n",
      "           8       0.36      0.35      0.36        23\n",
      "           9       0.29      0.19      0.23        21\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       200\n",
      "   macro avg       0.48      0.49      0.47       200\n",
      "weighted avg       0.47      0.47      0.46       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_logreg = model_logreg.predict(x_test)\n",
    "print(classification_report(y_test, y_predict_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous models, we have an average of precision of 0.47. We can try to make some modifications to the preprocessing (more number of mel coefficients for example, as we are using only 12 and the default is 20) to see if we can achieve better metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
